{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ea8dd1-a9da-46a4-acb6-cb493f59b0ae",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "\n",
    "Take all the new JSON files and preprocess them:\n",
    "- break apart logs to get pokemon used, moves, items, tera\n",
    "- label winner\n",
    "- rating change\n",
    "\n",
    "At the end, the dataframe will be written to parquet files.\n",
    "\n",
    "We also track processed battle ids, and filter them out early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f7ac5-738e-4059-8868-c2876ad2595e",
   "metadata": {},
   "source": [
    "### Checking Versions\n",
    "\n",
    "First, let's check which versions of Python and Spark we're using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df208fb8-127b-45d0-a35f-4ed4f3add2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started at: 2025-03-24 03:13:03\n",
      "Python version: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0]\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Print Python and Spark versions\n",
    "import sys\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Script started at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")\n",
    "\n",
    "# Create a new SparkSession with more robust timeout settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Showdown-Replay-Exploration\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.network.timeout\", \"3600s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"600s\") \\\n",
    "    .config(\"spark.sql.mapKeyDedupPolicy\", \"LAST_WIN\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0a0c4-4a23-496f-a0f5-35312ef92b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5db4762-c59e-4f89-83ad-8c57817e3573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting consolidation at: 2025-03-24 00:54:20\n",
      "Found 103 day directories to process\n",
      "Processing 2024-12-01...\n",
      "  Found 250 JSON files\n",
      "  Saved 250 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-01.parquet\n",
      "Processing 2024-12-02...\n",
      "  Found 689 JSON files\n",
      "  Saved 689 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-02.parquet\n",
      "Processing 2024-12-03...\n",
      "  Found 526 JSON files\n",
      "  Saved 526 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-03.parquet\n",
      "Processing 2024-12-04...\n",
      "  Found 531 JSON files\n",
      "  Saved 531 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-04.parquet\n",
      "Processing 2024-12-05...\n",
      "  Found 405 JSON files\n",
      "  Saved 405 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-05.parquet\n",
      "Processing 2024-12-06...\n",
      "  Found 419 JSON files\n",
      "  Saved 419 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-06.parquet\n",
      "Processing 2024-12-07...\n",
      "  Found 352 JSON files\n",
      "  Saved 352 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-07.parquet\n",
      "Processing 2024-12-08...\n",
      "  Found 401 JSON files\n",
      "  Saved 401 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-08.parquet\n",
      "Processing 2024-12-09...\n",
      "  Found 443 JSON files\n",
      "  Saved 443 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-09.parquet\n",
      "Processing 2024-12-10...\n",
      "  Found 539 JSON files\n",
      "  Saved 539 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-10.parquet\n",
      "Processing 2024-12-11...\n",
      "  Found 429 JSON files\n",
      "  Saved 429 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-11.parquet\n",
      "Processing 2024-12-12...\n",
      "  Found 1106 JSON files\n",
      "  Saved 1106 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-12.parquet\n",
      "Processing 2024-12-13...\n",
      "  Found 1510 JSON files\n",
      "  Saved 1510 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-13.parquet\n",
      "Processing 2024-12-14...\n",
      "  Found 1365 JSON files\n",
      "  Saved 1365 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-14.parquet\n",
      "Processing 2024-12-15...\n",
      "  Found 1615 JSON files\n",
      "  Saved 1615 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-15.parquet\n",
      "Processing 2024-12-16...\n",
      "  Found 1861 JSON files\n",
      "  Saved 1861 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-16.parquet\n",
      "Processing 2024-12-17...\n",
      "  Found 1849 JSON files\n",
      "  Saved 1849 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-17.parquet\n",
      "Processing 2024-12-18...\n",
      "  Found 1988 JSON files\n",
      "  Saved 1988 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-18.parquet\n",
      "Processing 2024-12-19...\n",
      "  Found 1919 JSON files\n",
      "  Saved 1919 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-19.parquet\n",
      "Processing 2024-12-20...\n",
      "  Found 1738 JSON files\n",
      "  Saved 1738 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-20.parquet\n",
      "Processing 2024-12-21...\n",
      "  Found 1794 JSON files\n",
      "  Saved 1794 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-21.parquet\n",
      "Processing 2024-12-22...\n",
      "  Found 1314 JSON files\n",
      "  Saved 1314 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-22.parquet\n",
      "Processing 2024-12-23...\n",
      "  Found 1971 JSON files\n",
      "  Saved 1971 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-23.parquet\n",
      "Processing 2024-12-24...\n",
      "  Found 2118 JSON files\n",
      "  Saved 2118 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-24.parquet\n",
      "Processing 2024-12-25...\n",
      "  Found 1695 JSON files\n",
      "  Saved 1695 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-25.parquet\n",
      "Processing 2024-12-26...\n",
      "  Found 1880 JSON files\n",
      "  Saved 1880 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-26.parquet\n",
      "Processing 2024-12-27...\n",
      "  Found 1671 JSON files\n",
      "  Saved 1671 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-27.parquet\n",
      "Processing 2024-12-28...\n",
      "  Found 1915 JSON files\n",
      "  Saved 1915 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-28.parquet\n",
      "Processing 2024-12-29...\n",
      "  Found 2044 JSON files\n",
      "  Saved 2044 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-29.parquet\n",
      "Processing 2024-12-30...\n",
      "  Found 2028 JSON files\n",
      "  Saved 2028 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-30.parquet\n",
      "Processing 2024-12-31...\n",
      "  Found 2268 JSON files\n",
      "  Saved 2268 records to ../../data/replays/gen9vgc2025regg/consolidated/2024-12-31.parquet\n",
      "Processing 2025-01-01...\n",
      "  Found 1963 JSON files\n",
      "  Saved 1963 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-01.parquet\n",
      "Processing 2025-01-02...\n",
      "  Found 2592 JSON files\n",
      "  Saved 2592 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-02.parquet\n",
      "Processing 2025-01-03...\n",
      "  Found 2993 JSON files\n",
      "  Saved 2993 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-03.parquet\n",
      "Processing 2025-01-04...\n",
      "  Found 2635 JSON files\n",
      "  Saved 2635 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-04.parquet\n",
      "Processing 2025-01-05...\n",
      "  Found 2474 JSON files\n",
      "  Saved 2474 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-05.parquet\n",
      "Processing 2025-01-06...\n",
      "  Found 2686 JSON files\n",
      "  Saved 2686 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-06.parquet\n",
      "Processing 2025-01-07...\n",
      "  Found 2823 JSON files\n",
      "  Saved 2823 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-07.parquet\n",
      "Processing 2025-01-08...\n",
      "  Found 2779 JSON files\n",
      "  Saved 2779 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-08.parquet\n",
      "Processing 2025-01-09...\n",
      "  Found 2824 JSON files\n",
      "  Saved 2824 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-09.parquet\n",
      "Processing 2025-01-10...\n",
      "  Found 2633 JSON files\n",
      "  Saved 2633 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-10.parquet\n",
      "Processing 2025-01-11...\n",
      "  Found 2390 JSON files\n",
      "  Saved 2390 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-11.parquet\n",
      "Processing 2025-01-12...\n",
      "  Found 3061 JSON files\n",
      "  Saved 3061 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-12.parquet\n",
      "Processing 2025-01-13...\n",
      "  Found 2741 JSON files\n",
      "  Saved 2741 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-13.parquet\n",
      "Processing 2025-01-14...\n",
      "  Found 2928 JSON files\n",
      "  Saved 2928 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-14.parquet\n",
      "Processing 2025-01-15...\n",
      "  Found 2531 JSON files\n",
      "  Saved 2531 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-15.parquet\n",
      "Processing 2025-01-16...\n",
      "  Found 2497 JSON files\n",
      "  Saved 2497 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-16.parquet\n",
      "Processing 2025-01-17...\n",
      "  Found 2431 JSON files\n",
      "  Saved 2431 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-17.parquet\n",
      "Processing 2025-01-18...\n",
      "  Found 2336 JSON files\n",
      "  Saved 2336 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-18.parquet\n",
      "Processing 2025-01-19...\n",
      "  Found 2503 JSON files\n",
      "  Saved 2503 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-19.parquet\n",
      "Processing 2025-01-20...\n",
      "  Found 2969 JSON files\n",
      "  Saved 2969 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-20.parquet\n",
      "Processing 2025-01-21...\n",
      "  Found 2721 JSON files\n",
      "  Saved 2721 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-21.parquet\n",
      "Processing 2025-01-22...\n",
      "  Found 2716 JSON files\n",
      "  Saved 2716 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-22.parquet\n",
      "Processing 2025-01-23...\n",
      "  Found 2757 JSON files\n",
      "  Saved 2757 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-23.parquet\n",
      "Processing 2025-01-24...\n",
      "  Found 2778 JSON files\n",
      "  Saved 2778 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-24.parquet\n",
      "Processing 2025-01-25...\n",
      "  Found 2438 JSON files\n",
      "  Saved 2438 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-25.parquet\n",
      "Processing 2025-01-26...\n",
      "  Found 2710 JSON files\n",
      "  Saved 2710 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-26.parquet\n",
      "Processing 2025-01-27...\n",
      "  Found 2918 JSON files\n",
      "  Saved 2918 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-27.parquet\n",
      "Processing 2025-01-28...\n",
      "  Found 2928 JSON files\n",
      "  Saved 2928 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-28.parquet\n",
      "Processing 2025-01-29...\n",
      "  Found 2869 JSON files\n",
      "  Saved 2869 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-29.parquet\n",
      "Processing 2025-01-30...\n",
      "  Found 2859 JSON files\n",
      "  Saved 2859 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-30.parquet\n",
      "Processing 2025-01-31...\n",
      "  Found 2548 JSON files\n",
      "  Saved 2548 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-01-31.parquet\n",
      "Processing 2025-02-01...\n",
      "  Found 2500 JSON files\n",
      "  Saved 2500 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-01.parquet\n",
      "Processing 2025-02-02...\n",
      "  Found 2547 JSON files\n",
      "  Saved 2547 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-02.parquet\n",
      "Processing 2025-02-03...\n",
      "  Found 2741 JSON files\n",
      "  Saved 2741 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-03.parquet\n",
      "Processing 2025-02-04...\n",
      "  Found 2905 JSON files\n",
      "  Saved 2905 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-04.parquet\n",
      "Processing 2025-02-05...\n",
      "  Found 2475 JSON files\n",
      "  Saved 2475 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-05.parquet\n",
      "Processing 2025-02-06...\n",
      "  Found 2359 JSON files\n",
      "  Saved 2359 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-06.parquet\n",
      "Processing 2025-02-07...\n",
      "  Found 2662 JSON files\n",
      "  Saved 2662 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-07.parquet\n",
      "Processing 2025-02-08...\n",
      "  Found 2360 JSON files\n",
      "  Saved 2360 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-08.parquet\n",
      "Processing 2025-02-09...\n",
      "  Found 2563 JSON files\n",
      "  Saved 2563 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-09.parquet\n",
      "Processing 2025-02-10...\n",
      "  Found 2519 JSON files\n",
      "  Saved 2519 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-10.parquet\n",
      "Processing 2025-02-11...\n",
      "  Found 2704 JSON files\n",
      "  Saved 2704 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-11.parquet\n",
      "Processing 2025-02-12...\n",
      "  Found 2367 JSON files\n",
      "  Saved 2367 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-12.parquet\n",
      "Processing 2025-02-13...\n",
      "  Found 2691 JSON files\n",
      "  Saved 2691 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-13.parquet\n",
      "Processing 2025-02-14...\n",
      "  Found 2621 JSON files\n",
      "  Saved 2621 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-14.parquet\n",
      "Processing 2025-02-15...\n",
      "  Found 2398 JSON files\n",
      "  Saved 2398 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-15.parquet\n",
      "Processing 2025-02-16...\n",
      "  Found 2605 JSON files\n",
      "  Saved 2605 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-16.parquet\n",
      "Processing 2025-02-17...\n",
      "  Found 2682 JSON files\n",
      "  Saved 2682 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-17.parquet\n",
      "Processing 2025-02-18...\n",
      "  Found 2902 JSON files\n",
      "  Saved 2902 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-18.parquet\n",
      "Processing 2025-02-19...\n",
      "  Found 2893 JSON files\n",
      "  Saved 2893 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-19.parquet\n",
      "Processing 2025-02-20...\n",
      "  Found 2710 JSON files\n",
      "  Saved 2710 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-20.parquet\n",
      "Processing 2025-02-21...\n",
      "  Found 2705 JSON files\n",
      "  Saved 2705 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-21.parquet\n",
      "Processing 2025-02-22...\n",
      "  Found 2410 JSON files\n",
      "  Saved 2410 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-22.parquet\n",
      "Processing 2025-02-23...\n",
      "  Found 2093 JSON files\n",
      "  Saved 2093 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-23.parquet\n",
      "Processing 2025-02-24...\n",
      "  Found 3221 JSON files\n",
      "  Saved 3221 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-24.parquet\n",
      "Processing 2025-02-25...\n",
      "  Found 3198 JSON files\n",
      "  Saved 3198 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-25.parquet\n",
      "Processing 2025-02-26...\n",
      "  Found 2918 JSON files\n",
      "  Saved 2918 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-26.parquet\n",
      "Processing 2025-02-27...\n",
      "  Found 3040 JSON files\n",
      "  Saved 3040 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-27.parquet\n",
      "Processing 2025-02-28...\n",
      "  Found 2896 JSON files\n",
      "  Saved 2896 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-02-28.parquet\n",
      "Processing 2025-03-01...\n",
      "  Found 2677 JSON files\n",
      "  Saved 2677 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-01.parquet\n",
      "Processing 2025-03-02...\n",
      "  Found 2848 JSON files\n",
      "  Saved 2848 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-02.parquet\n",
      "Processing 2025-03-03...\n",
      "  Found 2955 JSON files\n",
      "  Saved 2955 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-03.parquet\n",
      "Processing 2025-03-04...\n",
      "  Found 3133 JSON files\n",
      "  Saved 3133 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-04.parquet\n",
      "Processing 2025-03-05...\n",
      "  Found 3074 JSON files\n",
      "  Saved 3074 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-05.parquet\n",
      "Processing 2025-03-06...\n",
      "  Found 3042 JSON files\n",
      "  Saved 3042 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-06.parquet\n",
      "Processing 2025-03-07...\n",
      "  Found 2803 JSON files\n",
      "  Saved 2803 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-07.parquet\n",
      "Processing 2025-03-08...\n",
      "  Found 2794 JSON files\n",
      "  Saved 2794 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-08.parquet\n",
      "Processing 2025-03-09...\n",
      "  Found 3547 JSON files\n",
      "  Saved 3547 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-09.parquet\n",
      "Processing 2025-03-10...\n",
      "  Found 3335 JSON files\n",
      "  Saved 3335 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-10.parquet\n",
      "Processing 2025-03-11...\n",
      "  Found 3373 JSON files\n",
      "  Saved 3373 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-11.parquet\n",
      "Processing 2025-03-12...\n",
      "  Found 3412 JSON files\n",
      "  Saved 3412 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-12.parquet\n",
      "Processing 2025-03-13...\n",
      "  Found 180 JSON files\n",
      "  Saved 180 records to ../../data/replays/gen9vgc2025regg/consolidated/2025-03-13.parquet\n",
      "Consolidation complete! Processed 235522 files\n",
      "Total time: 459.53 seconds\n"
     ]
    }
   ],
   "source": [
    "# Save this as consolidate.py and run it once before running your notebook\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define paths\n",
    "input_dir = \"../../data/replays/gen9vgc2025regg/raw\"\n",
    "output_dir = \"../../data/replays/gen9vgc2025regg/consolidated\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Starting consolidation at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Process data by day\n",
    "day_dirs = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]\n",
    "print(f\"Found {len(day_dirs)} day directories to process\")\n",
    "\n",
    "total_files = 0\n",
    "\n",
    "for day_dir in day_dirs:\n",
    "    day_path = os.path.join(input_dir, day_dir)\n",
    "    print(f\"Processing {day_dir}...\")\n",
    "    \n",
    "    # Get all JSON files in the directory\n",
    "    json_files = [f for f in os.listdir(day_path) if f.endswith('.json')]\n",
    "    if not json_files:\n",
    "        continue\n",
    "        \n",
    "    print(f\"  Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    # Gather all JSON data\n",
    "    all_data = []\n",
    "    for file_name in json_files:\n",
    "        file_path = os.path.join(day_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                all_data.append(json.load(f))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if all_data:\n",
    "        # Convert to pandas DataFrame\n",
    "        df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Save as parquet file\n",
    "        output_file = os.path.join(output_dir, f\"{day_dir}.parquet\")\n",
    "        df.to_parquet(output_file)\n",
    "        print(f\"  Saved {len(all_data)} records to {output_file}\")\n",
    "        \n",
    "        total_files += len(json_files)\n",
    "\n",
    "print(f\"Consolidation complete! Processed {total_files} files\")\n",
    "print(f\"Total time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39a921-a0de-491d-8e69-41ce32337556",
   "metadata": {},
   "source": [
    "### Step 2: Load in consolidated parquet and filter out already processed rows\n",
    "\n",
    "We will load only the Regulation G Json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e872dea-e3bc-4a8c-abca-40fff226bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previously processed IDs found. Processing all replays.\n"
     ]
    }
   ],
   "source": [
    "# Use spark to read files then partition after to speed up processing\n",
    "import os\n",
    "\n",
    "# Path to consolidated parquet files\n",
    "consolidated_path = \"../../data/replays/gen9vgc2025regg/consolidated\"\n",
    "\n",
    "# Read all parquet files from the consolidated directory\n",
    "logs_df = spark.read.parquet(consolidated_path)\n",
    "\n",
    "processed_ids_path = \"../../data/replays/gen9vgc2025regg/pre_processed/replay_ids/*\"\n",
    "\n",
    "# Skip the filtering step on first run\n",
    "try:\n",
    "    # Try to read processed IDs\n",
    "    processed_ids = spark.read.parquet(processed_ids_path)\n",
    "    logs_df = logs_df.join(processed_ids, \"id\", \"left_anti\")\n",
    "except:\n",
    "    # If it fails, just create the directory and continue\n",
    "    os.makedirs(os.path.dirname(processed_ids_path.replace(\"*\", \"\")), exist_ok=True)\n",
    "    print(\"No previously processed IDs found. Processing all replays.\")\n",
    "\n",
    "# Repartition\n",
    "logs_df = logs_df.repartition(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09663705-525b-450b-985e-67330c3288a1",
   "metadata": {},
   "source": [
    "### Step 3: Extract Relevant Information\n",
    "\n",
    "Player Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23554744-1106-49b1-afe8-147c90872a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, transform, expr, regexp_replace, col, explode, split, lit\n",
    "from pyspark.sql.functions import collect_list, struct, map_from_entries, collect_set, explode_outer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "\n",
    "# Extract player names and ratings\n",
    "def extract_player_info(df):\n",
    "    df = df.withColumn(\"player1\", logs_df[\"players\"].getItem(0))\n",
    "    df = df.withColumn(\"player2\", logs_df[\"players\"].getItem(1))\n",
    "    \n",
    "    # For player1_rating_before and player2_rating_before\n",
    "    df = df.withColumn(\"player1_rating_before\", regexp_extract(\"log\", r\"\\{\\}'s rating: (\\d+)\", 1))\n",
    "    df = df.withColumn(\"player2_rating_before\", regexp_extract(\"log\", r\"\\{\\}'s rating: (\\d+)\", 1))\n",
    "    \n",
    "    # For player1_rating_after and player2_rating_after\n",
    "    df = df.withColumn(\"player1_rating_after\", regexp_extract(\"log\", r\"\\{\\}'s rating: \\d+ → (\\d+)\", 1))\n",
    "    df = df.withColumn(\"player2_rating_after\", regexp_extract(\"log\", r\"\\{\\}'s rating: \\d+ → (\\d+)\", 1))\n",
    "\n",
    "df_step_1 = extract_player_info(logs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37882e2-d611-49af-9be9-8898c8683698",
   "metadata": {},
   "source": [
    "### Open Team Sheets\n",
    "\n",
    "Players can choose to allow open team sheets. This will give a preview of the entire team, items, moves, and tera types. We will extract these team sheets when available, and then add them back to the original data frames. We can use open team sheets to compare the rest of our extraction work for accuracy/extraction issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722a535f-6870-401a-8209-9a040aa83c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_team_sheets(df):\n",
    "    from pyspark.sql.functions import regexp_extract, transform, expr, regexp_replace, col, explode, split, lit\n",
    "    from pyspark.sql.functions import collect_list, struct, map_from_entries, when\n",
    "    \n",
    "    # Check if team sheets were accepted (presence of showteam indicates acceptance)\n",
    "    df_with_acceptance = df.withColumn(\n",
    "        \"team_sheets_accepted\", \n",
    "        when(col(\"log\").contains(\"|showteam|p\"), lit(True)).otherwise(lit(False))\n",
    "    )\n",
    "    \n",
    "    # Extract showteam information when accepted\n",
    "    df_with_team_info = df_with_acceptance.withColumn(\n",
    "        \"p1_team_details\", \n",
    "        when(col(\"log\").contains(\"|showteam|p\"), \n",
    "             regexp_extract(col(\"log\"), r\"\\|showteam\\|p1\\|(.*?)(?=\\n|\\|showteam\\|p2\\|)\", 1))\n",
    "        .otherwise(lit(None))\n",
    "    ).withColumn(\n",
    "        \"p2_team_details\", \n",
    "        when(col(\"log\").contains(\"|showteam|p2|\"), \n",
    "             regexp_extract(col(\"log\"), r\"\\|showteam\\|p2\\|(.*?)(?=\\n|\\|j\\||\\|inactive\\|)\", 1))\n",
    "        .otherwise(lit(None))\n",
    "    )\n",
    "    \n",
    "    # Process player 1 team details\n",
    "    df_p1 = df_with_team_info.filter(col(\"p1_team_details\").isNotNull())\n",
    "    # Split the team details into individual Pokémon entries\n",
    "    df_p1 = df_p1.withColumn(\"p1_team_entries\", split(col(\"p1_team_details\"), r\"\\]\"))\n",
    "    \n",
    "    # Explode the entries to process each Pokémon\n",
    "    df_p1_exploded = df_p1.select(\"id\", explode(\"p1_team_entries\").alias(\"p1_entry\"))\n",
    "    \n",
    "    # Extract details for each Pokémon\n",
    "    df_p1_pokemon = df_p1_exploded.withColumn(\n",
    "        \"pokemon\", regexp_extract(col(\"p1_entry\"), r\"^(.*?)\\|\\|\", 1)\n",
    "    ).withColumn(\n",
    "        \"item\", regexp_extract(col(\"p1_entry\"), r\"\\|\\|(.*?)\\|\", 1)\n",
    "    ).withColumn(\n",
    "        \"ability\", regexp_extract(col(\"p1_entry\"), r\"\\|([^|]+)\\|[^|]+,[^|]+,\", 1)\n",
    "    ).withColumn(\n",
    "        \"moves\", regexp_extract(col(\"p1_entry\"), r\"\\|([^|]+,[^|]+,[^|]+,[^|]+)\\|\", 1)\n",
    "    ).withColumn(\n",
    "        \"tera_type\", regexp_extract(col(\"p1_entry\"), r\",,,,,([^,\\]]+)\", 1)\n",
    "    )\n",
    "    \n",
    "    # Filter out empty entries\n",
    "    df_p1_pokemon = df_p1_pokemon.filter(col(\"pokemon\").isNotNull() & (col(\"pokemon\") != \"\"))\n",
    "    \n",
    "    # Create a map of Pokémon to their details\n",
    "    p1_pokemon_map = df_p1_pokemon.groupBy(\"id\").agg(\n",
    "        map_from_entries(\n",
    "            collect_list(\n",
    "                struct(\n",
    "                    col(\"pokemon\"), \n",
    "                    struct(\n",
    "                        col(\"item\"), \n",
    "                        col(\"ability\"), \n",
    "                        col(\"moves\"), \n",
    "                        col(\"tera_type\")\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ).alias(\"p1_pokemon_data_open_team\")\n",
    "    )\n",
    "    \n",
    "    # Join back to the main dataframe\n",
    "    df_with_team_info = df_with_team_info.join(p1_pokemon_map, \"id\", \"left\")\n",
    "\n",
    "    \n",
    "    # Process player 2 team details\n",
    "    df_p2 = df_with_team_info.filter(col(\"p2_team_details\").isNotNull())\n",
    "\n",
    "    # Split the team details into individual Pokémon entries\n",
    "    df_p2 = df_p2.withColumn(\"p2_team_entries\", split(col(\"p2_team_details\"), r\"\\]\"))\n",
    "    \n",
    "    # Explode the entries to process each Pokémon\n",
    "    df_p2_exploded = df_p2.select(\"id\", explode(\"p2_team_entries\").alias(\"p2_entry\"))\n",
    "    \n",
    "    # Extract details for each Pokémon\n",
    "    df_p2_pokemon = df_p2_exploded.withColumn(\n",
    "        \"pokemon\", regexp_extract(col(\"p2_entry\"), r\"^(.*?)\\|\\|\", 1)\n",
    "    ).withColumn(\n",
    "        \"item\", regexp_extract(col(\"p2_entry\"), r\"\\|\\|(.*?)\\|\", 1)\n",
    "    ).withColumn(\n",
    "        \"ability\", regexp_extract(col(\"p2_entry\"), r\"\\|([^|]+)\\|[^|]+,[^|]+,\", 1)\n",
    "    ).withColumn(\n",
    "        \"moves\", regexp_extract(col(\"p2_entry\"), r\"\\|([^|]+,[^|]+,[^|]+,[^|]+)\\|\", 1)\n",
    "    ).withColumn(\n",
    "        \"tera_type\", regexp_extract(col(\"p2_entry\"), r\",,,,,([^,\\]]+)\", 1)\n",
    "    )\n",
    "    \n",
    "    # Filter out empty entries\n",
    "    df_p2_pokemon = df_p2_pokemon.filter(col(\"pokemon\").isNotNull() & (col(\"pokemon\") != \"\"))\n",
    "    \n",
    "    # Create a map of Pokémon to their details\n",
    "    p2_pokemon_map = df_p2_pokemon.groupBy(\"id\").agg(\n",
    "        map_from_entries(\n",
    "            collect_list(\n",
    "                struct(\n",
    "                    col(\"pokemon\"), \n",
    "                    struct(\n",
    "                        col(\"item\"), \n",
    "                        col(\"ability\"), \n",
    "                        col(\"moves\"), \n",
    "                        col(\"tera_type\")\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ).alias(\"p2_pokemon_data_open_team\")\n",
    "    )\n",
    "    \n",
    "    # Join back to the main dataframe\n",
    "    df_with_team_info = df_with_team_info.join(p2_pokemon_map, \"id\", \"left\")\n",
    "\n",
    "    # Drop the temporary columns before returning\n",
    "    final_df = df_with_team_info.drop(\"p1_team_details\", \"p2_team_details\")\n",
    "    \n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Call the function\n",
    "df_step_2 = analyze_team_sheets(logs_df)\n",
    "\n",
    "# Call the function\n",
    "# logs_df2 = analyze_team_sheets(logs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df476909-d0f1-42bd-8683-dd9adc267812",
   "metadata": {},
   "source": [
    "Extract logs for Pokémon, Moves, and Items- we will process these logs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea974ee-4cec-4ea6-9086-b479739b1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data we will use several times later on\n",
    "def extract_raw_data(df):\n",
    "    df = df.withColumn(\"player1_pokemon_array\", \n",
    "                                 expr(\"regexp_extract_all(log, '\\\\\\\\|poke\\\\\\\\|p1\\\\\\\\|([^\\\\\\\\|]+)', 1)\"))\n",
    "    df = df.withColumn(\"player2_pokemon_array\", \n",
    "                                 expr(\"regexp_extract_all(log, '\\\\\\\\|poke\\\\\\\\|p2\\\\\\\\|([^\\\\\\\\|]+)', 1)\"))\n",
    "    \n",
    "    # Then clean each array element to keep only the Pokémon name\n",
    "    df = df.withColumn(\"player1_pokemon_array\", \n",
    "                                 transform(\"player1_pokemon_array\", lambda x: regexp_replace(x, \", L\\\\d+.*\", \"\")))\n",
    "    df = df.withColumn(\"player2_pokemon_array\", \n",
    "                                 transform(\"player2_pokemon_array\", lambda x: regexp_replace(x, \", L\\\\d+.*\", \"\")))\n",
    "    \n",
    "    # Extract full team preview for both players\n",
    "    df = df.withColumn(\"player1_full_team_raw\", \n",
    "                               expr(\"regexp_extract_all(log, '\\\\\\\\|poke\\\\\\\\|p1\\\\\\\\|([^,]+)', 1)\"))\n",
    "    df = df.withColumn(\"player2_full_team_raw\", \n",
    "                               expr(\"regexp_extract_all(log, '\\\\\\\\|poke\\\\\\\\|p2\\\\\\\\|([^,]+)', 1)\"))\n",
    "    \n",
    "    # Clean the team arrays to handle any special forms (like Urshifu-*)\n",
    "    df = df.withColumn(\"player1_full_team\", \n",
    "                               transform(\"player1_full_team_raw\", lambda x: regexp_replace(x, \"\\\\-\\\\*\", \"\")))\n",
    "    df = df.withColumn(\"player2_full_team\", \n",
    "                               transform(\"player2_full_team_raw\", lambda x: regexp_replace(x, \"\\\\-\\\\*\", \"\")))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract all moves used by each player's Pokémon (will parse this later)\n",
    "    df = df.withColumn(\"player1_moves_raw\", \n",
    "                                 expr(\"regexp_extract_all(log, '\\\\\\\\|move\\\\\\\\|p1[ab]: ([^\\\\\\\\|]+)\\\\\\\\|([^\\\\\\\\|]+)', 0)\"))\n",
    "    df = df.withColumn(\"player2_moves_raw\", \n",
    "                                 expr(\"regexp_extract_all(log, '\\\\\\\\|move\\\\\\\\|p2[ab]: ([^\\\\\\\\|]+)\\\\\\\\|([^\\\\\\\\|]+)', 0)\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df_step_3 = extract_raw_data(df_step_2)\n",
    "\n",
    "# pandas_df_preview2 = logs_df_with_teamsheet_raw.limit(15).toPandas()\n",
    "\n",
    "# pandas_df_preview2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074dfac8-eff3-4432-9b5a-2ecc278bf481",
   "metadata": {},
   "source": [
    "Weather, Forfeits, Status Effects, Winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31c5e78-5d00-43ff-a3e5-d6cb02f48e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weather, status, forfeit, winner\n",
    "def extract_weather_status_forfeit_winner_changes(df):\n",
    "    df = df.withColumn(\"weather\", regexp_extract(\"log\", r\"\\|weather\\|([^\\|]+)\", 1))\n",
    "    \n",
    "    # Extract forfeit information\n",
    "    df = df.withColumn(\"forfeit\", regexp_extract(\"log\", r\"\\|message\\|([^\\|]+ forfeited)\", 1))\n",
    "    \n",
    "    # Extract status effects like confusion, sleep, etc.\n",
    "    df = df.withColumn(\"status_effects\", regexp_extract(\"log\", r\"\\|start\\|p1a: [^\\|]+\\|([^\\|]+)\", 1))\n",
    "\n",
    "    df = df.withColumn(\"winner\", regexp_extract(\"log\", r\"\\|win\\|([^\\|]+)\", 1))\n",
    "\n",
    "    return df\n",
    "\n",
    "df_step_4 = extract_weather_status_forfeit_winner_changes(df_step_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee380c3-953e-4334-bb52-128bda47d9e5",
   "metadata": {},
   "source": [
    "### Extract Pokemon Moves\n",
    "\n",
    "Previously we built a column with all the data for a pokemon and its moves.\n",
    "\n",
    "Now we will create a new df for the moves.\n",
    "\n",
    "1. Explode the moves ie create a new row for each move\n",
    "2. Extract the pokemon for each move, and its moves\n",
    "3. Group each pokemon by the battle it took place in (because we seperated the dfs into a player 1 and player 2 we don't need to worry about 2 players using the same pokemon accidentally being joined together)\n",
    "4. Convert to a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bb00be5-136d-40f0-8a8a-7d6a5dc6ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, transform, expr, regexp_replace, col, explode, split, lit\n",
    "from pyspark.sql.functions import collect_list, struct, map_from_entries, collect_set\n",
    "\n",
    "def extract_pokemon_moves(df):\n",
    "    # Explode the arrays to work with individual move patterns\n",
    "    p1_moves_df = df.select(\"id\", explode(\"player1_moves_raw\").alias(\"move_pattern\"))\n",
    "    p2_moves_df = df.select(\"id\", explode(\"player2_moves_raw\").alias(\"move_pattern\"))\n",
    "    \n",
    "    # Extract Pokémon and move from the patterns using regexp_extract\n",
    "    p1_moves_df = p1_moves_df.withColumn(\"pokemon\", \n",
    "                                         regexp_extract(\"move_pattern\", \"\\\\|move\\\\|p1[ab]: ([^\\\\|]+)\\\\|\", 1))\n",
    "    p1_moves_df = p1_moves_df.withColumn(\"move\", \n",
    "                                         regexp_extract(\"move_pattern\", \"\\\\|move\\\\|p1[ab]: [^\\\\|]+\\\\|([^\\\\|]+)\", 1))\n",
    "    \n",
    "    p2_moves_df = p2_moves_df.withColumn(\"pokemon\", \n",
    "                                         regexp_extract(\"move_pattern\", \"\\\\|move\\\\|p2[ab]: ([^\\\\|]+)\\\\|\", 1))\n",
    "    p2_moves_df = p2_moves_df.withColumn(\"move\", \n",
    "                                         regexp_extract(\"move_pattern\", \"\\\\|move\\\\|p2[ab]: [^\\\\|]+\\\\|([^\\\\|]+)\", 1))\n",
    "    \n",
    "    # Group by battle ID and Pokémon to get all moves used by each Pokémon. \n",
    "    # We use set to remove duplicates (ie pokemon using same move more than once per match)\n",
    "    p1_moves_by_pokemon = p1_moves_df.groupBy(\"id\", \"pokemon\").agg(collect_set(\"move\").alias(\"moves\"))\n",
    "    p2_moves_by_pokemon = p2_moves_df.groupBy(\"id\", \"pokemon\").agg(collect_set(\"move\").alias(\"moves\"))\n",
    "    \n",
    "    # Convert to a map structure for easier joining\n",
    "    p1_moves_map = p1_moves_by_pokemon.groupBy(\"id\").agg(\n",
    "        map_from_entries(collect_list(struct(\"pokemon\", \"moves\"))).alias(\"player1_pokemon_moves\")\n",
    "    )\n",
    "    p2_moves_map = p2_moves_by_pokemon.groupBy(\"id\").agg(\n",
    "        map_from_entries(collect_list(struct(\"pokemon\", \"moves\"))).alias(\"player2_pokemon_moves\")\n",
    "    )\n",
    "    \n",
    "    # Join back to the main dataframe\n",
    "    df = df.join(p1_moves_map, \"id\", \"left\")\n",
    "    df = df.join(p2_moves_map, \"id\", \"left\")\n",
    "\n",
    "    # drop moves raw since we don't need it anymore\n",
    "    df = df.drop('player1_moves_raw', 'player2_moves_raw')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_step_5 = extract_pokemon_moves(df_step_4)\n",
    "\n",
    "# pandas_df_preview4 = df_semi_final.limit(15).toPandas()\n",
    "\n",
    "# pandas_df_preview4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c6e61-3d6e-488e-a159-43f20083056f",
   "metadata": {},
   "source": [
    "#### Cache the logs df so we don't have to do these expensive computes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03b618cc-4b5e-4a21-8320-9b50905ac15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the DataFrame after the expensive move mapping operations\n",
    "# df_step_5 = df_step_5.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c146771-5e6a-426e-9a88-67ef62406968",
   "metadata": {},
   "source": [
    "### Process Items\n",
    "\n",
    "Each pokemon holds an item. This item is not revealed unless some game event leads to it being revealed.\n",
    "\n",
    "To solve this, we parse all item logs to match items to a pokemon. This could be an item knocked off mid turn, or an end turn item useage like berry consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448d48d8-ca75-4a60-8d03-48e5f7b5e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import regexp_extract, transform, expr, regexp_replace, col, explode, split, lit\n",
    "from pyspark.sql.functions import collect_list, struct, map_from_entries, collect_set, explode_outer\n",
    "\n",
    "# Process both enditem events and item events\n",
    "\n",
    "def extract_items(df):\n",
    "    # First, extract all item-related patterns from the logs\n",
    "    df = df.withColumn(\"player1_item_patterns\", \n",
    "                               expr(\"regexp_extract_all(log, '\\\\\\\\|-item\\\\\\\\|p1[ab]: ([^\\\\\\\\|]+)\\\\\\\\|([^\\\\\\\\|]+)', 0)\"))\n",
    "    df = df.withColumn(\"player2_item_patterns\", \n",
    "                               expr(\"regexp_extract_all(log, '\\\\\\\\|-item\\\\\\\\|p2[ab]: ([^\\\\\\\\|]+)\\\\\\\\|([^\\\\\\\\|]+)', 0)\"))\n",
    "    \n",
    "    # Also capture enditem events which show when items are consumed/lost\n",
    "    df = df.withColumn(\"player1_enditem_patterns\", \n",
    "                               expr(\"regexp_extract_all(log, '\\\\\\\\|-enditem\\\\\\\\|p1[ab]: ([^\\\\\\\\|]+)\\\\\\\\|([^\\\\\\\\|]+)', 0)\"))\n",
    "    df = df.withColumn(\"player2_enditem_patterns\", \n",
    "                               expr(\"regexp_extract_all(log, '\\\\\\\\|-enditem\\\\\\\\|p2[ab]: ([^\\\\\\\\|]+)\\\\\\\\|([^\\\\\\\\|]+)', 0)\"))\n",
    "    \n",
    "    # Process item events for player 1\n",
    "    p1_item_df = df.select(\"id\", explode_outer(\"player1_item_patterns\").alias(\"item_pattern\"))\n",
    "    p1_item_df = p1_item_df.withColumn(\"pokemon\", \n",
    "                                     regexp_extract(\"item_pattern\", \"\\\\|-item\\\\|p1[ab]: ([^\\\\|]+)\\\\|\", 1))\n",
    "    p1_item_df = p1_item_df.withColumn(\"item\", \n",
    "                                     regexp_extract(\"item_pattern\", \"\\\\|-item\\\\|p1[ab]: [^\\\\|]+\\\\|([^\\\\|]+)\", 1))\n",
    "    \n",
    "    # Process enditem events for player 1\n",
    "    p1_enditem_df = df.select(\"id\", explode_outer(\"player1_enditem_patterns\").alias(\"enditem_pattern\"))\n",
    "    p1_enditem_df = p1_enditem_df.withColumn(\"pokemon\", \n",
    "                                           regexp_extract(\"enditem_pattern\", \"\\\\|-enditem\\\\|p1[ab]: ([^\\\\|]+)\\\\|\", 1))\n",
    "    p1_enditem_df = p1_enditem_df.withColumn(\"item\", \n",
    "                                           regexp_extract(\"enditem_pattern\", \"\\\\|-enditem\\\\|p1[ab]: [^\\\\|]+\\\\|([^\\\\|]+)\", 1))\n",
    "    \n",
    "    # Process item events for player 2\n",
    "    p2_item_df = df.select(\"id\", explode_outer(\"player2_item_patterns\").alias(\"item_pattern\"))\n",
    "    p2_item_df = p2_item_df.withColumn(\"pokemon\", \n",
    "                                     regexp_extract(\"item_pattern\", \"\\\\|-item\\\\|p2[ab]: ([^\\\\|]+)\\\\|\", 1))\n",
    "    p2_item_df = p2_item_df.withColumn(\"item\", \n",
    "                                     regexp_extract(\"item_pattern\", \"\\\\|-item\\\\|p2[ab]: [^\\\\|]+\\\\|([^\\\\|]+)\", 1))\n",
    "    \n",
    "    # Process enditem events for player 2\n",
    "    p2_enditem_df = df.select(\"id\", explode_outer(\"player2_enditem_patterns\").alias(\"enditem_pattern\"))\n",
    "    p2_enditem_df = p2_enditem_df.withColumn(\"pokemon\", \n",
    "                                           regexp_extract(\"enditem_pattern\", \"\\\\|-enditem\\\\|p2[ab]: ([^\\\\|]+)\\\\|\", 1))\n",
    "    p2_enditem_df = p2_enditem_df.withColumn(\"item\", \n",
    "                                           regexp_extract(\"enditem_pattern\", \"\\\\|-enditem\\\\|p2[ab]: [^\\\\|]+\\\\|([^\\\\|]+)\", 1))\n",
    "    \n",
    "    # Union the item and enditem dataframes for each player\n",
    "    p1_all_items_df = p1_item_df.union(p1_enditem_df)\n",
    "    p2_all_items_df = p2_item_df.union(p2_enditem_df)\n",
    "    \n",
    "    # Group by battle ID and Pokémon to get items used by each Pokémon\n",
    "    p1_items_by_pokemon = p1_all_items_df.filter(col(\"pokemon\").isNotNull() & col(\"item\").isNotNull()) \\\n",
    "                                        .groupBy(\"id\", \"pokemon\").agg(collect_set(\"item\").alias(\"items\"))\n",
    "    p2_items_by_pokemon = p2_all_items_df.filter(col(\"pokemon\").isNotNull() & col(\"item\").isNotNull()) \\\n",
    "                                        .groupBy(\"id\", \"pokemon\").agg(collect_set(\"item\").alias(\"items\"))\n",
    "    \n",
    "    # Convert to a map structure for easier joining\n",
    "    p1_items_map = p1_items_by_pokemon.groupBy(\"id\").agg(\n",
    "        map_from_entries(collect_list(struct(\"pokemon\", \"items\"))).alias(\"player1_pokemon_items\")\n",
    "    )\n",
    "    p2_items_map = p2_items_by_pokemon.groupBy(\"id\").agg(\n",
    "        map_from_entries(collect_list(struct(\"pokemon\", \"items\"))).alias(\"player2_pokemon_items\")\n",
    "    )\n",
    "    \n",
    "    # Join back to the main dataframe\n",
    "    df = df.join(p1_items_map, \"id\", \"left\")\n",
    "    df = df.join(p2_items_map, \"id\", \"left\")\n",
    "\n",
    "    df = df.drop('player1_item_patterns', 'player2_item_patterns', 'player1_enditem_patterns', 'player2_enditem_patterns')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_step_6 = extract_items(df_step_5)\n",
    "\n",
    "# pandas_df_preview5 = df_semi_semi_final.limit(15).toPandas()\n",
    "\n",
    "# pandas_df_preview5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2bbbc-967e-4476-b9e6-5738b0258936",
   "metadata": {},
   "source": [
    "### Extracting Tera Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "784c425a-f8fa-4a36-808a-0d2b8b383021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, transform, expr, regexp_replace, col, explode, split, lit\n",
    "from pyspark.sql.functions import collect_list, struct, map_from_entries, collect_set, explode_outer\n",
    "\n",
    "# Process terastallize events for both players\n",
    "# First, extract all terastallize patterns from the logs\n",
    "def extract_tera(df):\n",
    "    df = df.withColumn(\"player1_tera_patterns\", \n",
    "                               expr(\"regexp_extract_all(log, '\\\\\\\\|-terastallize\\\\\\\\|p1[ab]: ([^\\\\\\\\|]+)\\\\\\\\|([^\\\\\\\\|]+)', 0)\"))\n",
    "    df = df.withColumn(\"player2_tera_patterns\", \n",
    "                               expr(\"regexp_extract_all(log, '\\\\\\\\|-terastallize\\\\\\\\|p2[ab]: ([^\\\\\\\\|]+)\\\\\\\\|([^\\\\\\\\|]+)', 0)\"))\n",
    "    \n",
    "    # Process terastallize events for player 1\n",
    "    p1_tera_df = df.select(\"id\", explode_outer(\"player1_tera_patterns\").alias(\"tera_pattern\"))\n",
    "    p1_tera_df = p1_tera_df.withColumn(\"pokemon\", \n",
    "                                     regexp_extract(\"tera_pattern\", \"\\\\|-terastallize\\\\|p1[ab]: ([^\\\\|]+)\\\\|\", 1))\n",
    "    p1_tera_df = p1_tera_df.withColumn(\"tera_type\", \n",
    "                                     regexp_extract(\"tera_pattern\", \"\\\\|-terastallize\\\\|p1[ab]: [^\\\\|]+\\\\|([^\\\\|\\\\n]+)\", 1))\n",
    "    \n",
    "    # Process terastallize events for player 2\n",
    "    p2_tera_df = df.select(\"id\", explode_outer(\"player2_tera_patterns\").alias(\"tera_pattern\"))\n",
    "    p2_tera_df = p2_tera_df.withColumn(\"pokemon\", \n",
    "                                     regexp_extract(\"tera_pattern\", \"\\\\|-terastallize\\\\|p2[ab]: ([^\\\\|]+)\\\\|\", 1))\n",
    "    p2_tera_df = p2_tera_df.withColumn(\"tera_type\", \n",
    "                                     regexp_extract(\"tera_pattern\", \"\\\\|-terastallize\\\\|p2[ab]: [^\\\\|]+\\\\|([^\\\\|\\\\n]+)\", 1))\n",
    "    \n",
    "    # Filter out null values and create maps\n",
    "    p1_tera_map = p1_tera_df.filter(col(\"pokemon\").isNotNull() & col(\"tera_type\").isNotNull()) \\\n",
    "                           .groupBy(\"id\") \\\n",
    "                           .agg(map_from_entries(collect_list(struct(\"pokemon\", \"tera_type\"))).alias(\"player1_pokemon_tera\"))\n",
    "    \n",
    "    p2_tera_map = p2_tera_df.filter(col(\"pokemon\").isNotNull() & col(\"tera_type\").isNotNull()) \\\n",
    "                           .groupBy(\"id\") \\\n",
    "                           .agg(map_from_entries(collect_list(struct(\"pokemon\", \"tera_type\"))).alias(\"player2_pokemon_tera\"))\n",
    "    \n",
    "    # Join back to the main dataframe\n",
    "    df = df.join(p1_tera_map, \"id\", \"left\")\n",
    "    df = df.join(p2_tera_map, \"id\", \"left\")\n",
    "\n",
    "    # drop raw patterns\n",
    "    df = df.drop('player1_tera_patterns', 'player2_tera_patterns')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_final = extract_tera(df_step_6)\n",
    "\n",
    "# pandas_df_preview6 = df_final.limit(15).toPandas()\n",
    "\n",
    "# pandas_df_preview6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d59d650-9c57-4401-9ab0-121698555b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame (only do this for small result sets!)\n",
    "\n",
    "# uncomment below to see preview\n",
    "# pandas_df = logs_df.limit(5).toPandas()\n",
    "\n",
    "# pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad70af8-73c3-442d-bdd4-a83130073da1",
   "metadata": {},
   "source": [
    "### Save to parquet\n",
    "\n",
    "We will end this exploration and save our results to parquet so we can reuse (and also put other functions so we can copy and paste them for later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "474d7742-12b4-4a3d-ae2c-55daaaf15f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to ../../data/replays/gen9vgc2025regg/pre_processed/replays/2025-03-24_03-13-20\n",
      "Saved processed IDs to ../../data/replays/gen9vgc2025regg/pre_processed/replay_ids/2025-03-24_03-13-20\n",
      "\n",
      "Processed 235522 replays in this run\n",
      "Script completed at: 2025-03-24 03:15:44\n",
      "Total execution time: 160.99 seconds (2.68 minutes)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create a timestamp for the processed data that includes time\n",
    "process_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define the output paths with the more specific timestamp\n",
    "processed_data_path = f\"../../data/replays/gen9vgc2025regg/pre_processed/replays/{process_datetime}\"\n",
    "processed_ids_path = f\"../../data/replays/gen9vgc2025regg/pre_processed/replay_ids/{process_datetime}\"\n",
    "\n",
    "df_final.cache()\n",
    "\n",
    "# Drop the log column to reduce storage size\n",
    "df_final_no_logs = df_final.drop(\"log\")\n",
    "\n",
    "# Save the full processed dataframe as parquet\n",
    "df_final_no_logs.write.mode(\"overwrite\").parquet(processed_data_path)\n",
    "print(f\"Saved processed data to {processed_data_path}\")\n",
    "\n",
    "# Save just the IDs of processed replays for future reference\n",
    "id_df = logs_df.select(\"id\")\n",
    "id_df.write.mode(\"overwrite\").parquet(processed_ids_path)\n",
    "print(f\"Saved processed IDs to {processed_ids_path}\")\n",
    "\n",
    "replay_count = logs_df.count()\n",
    "print(f\"\\nProcessed {replay_count} replays in this run\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Script completed at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}\")\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds ({execution_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e50f71ec-b3e0-4750-85bc-32c571c51853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to pandas dataframe and saving using pandas...\n",
      "Error with pandas approach: [Errno 111] Connection refused\n"
     ]
    }
   ],
   "source": [
    "# WORKAROUND FOR PY4J ERROR: Use pandas to save data instead of Spark\n",
    "# This approach collects data to the driver node and writes it using pandas\n",
    "# Note: This will only work if your dataset fits in driver memory\n",
    "\n",
    "\n",
    "# try:\n",
    "#     # Create output directories if they don't exist\n",
    "#     os.makedirs(os.path.dirname(processed_data_path), exist_ok=True)\n",
    "#     os.makedirs(os.path.dirname(processed_ids_path), exist_ok=True)\n",
    "    \n",
    "#     # Convert to pandas dataframe and save using pandas\n",
    "#     print(\"Converting to pandas dataframe and saving using pandas...\")\n",
    "    \n",
    "#     # Limit the records to avoid memory issues (adjust based on your data size)\n",
    "#     pandas_df = df_final_no_logs.limit(50000).toPandas()\n",
    "#     pandas_df.to_parquet(f\"{processed_data_path}_pandas.parquet\")\n",
    "    \n",
    "#     # Also save IDs\n",
    "#     id_pd_df = logs_df.select(\"id\").limit(50000).toPandas()\n",
    "#     id_pd_df.to_parquet(f\"{processed_ids_path}_pandas.parquet\")\n",
    "    \n",
    "#     print(f\"Successfully saved data using pandas to {processed_data_path}_pandas.parquet\")\n",
    "#     print(f\"Successfully saved IDs using pandas to {processed_ids_path}_pandas.parquet\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Error with pandas approach: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40475c8a-b9e4-4aa3-a4e5-9c69e4ea0ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce676e8-3eaa-444d-8719-714da4449ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
